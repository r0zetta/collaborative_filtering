{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note this is broken for pytorch 1.7.x, so please use pytorch 1.6\n",
    "# pip install torch==1.6.0 torchvision==0.7.0\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from fastai.tabular.all import *\n",
    "from fastai.collab import *\n",
    "\n",
    "import networkx as nx\n",
    "import community\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "# Helper functions\n",
    "def save_ratings(df, fn):\n",
    "    with open(fn, \"w\") as f:\n",
    "        f.write(\"Source,Target,Weight\\n\")\n",
    "        for item in zip(df[\"Source\"], df[\"Target\"], df[\"Weight\"]):\n",
    "            s, t, w = item\n",
    "            f.write(str(s)+\",\"+str(t)+\",\"+str(w)+\"\\n\")\n",
    "\n",
    "def key_with_max_value(d):  \n",
    "     v = list(d.values())\n",
    "     k = list(d.keys())\n",
    "     return k[v.index(max(v))], max(v)\n",
    "\n",
    "def key_with_min_value(d):  \n",
    "     v = list(d.values())\n",
    "     k = list(d.keys())\n",
    "     return k[v.index(min(v))], min(v)\n",
    "\n",
    "def get_most_similar(uid, matrix, max_matches):\n",
    "    if uid >= len(matrix):\n",
    "        return None\n",
    "    all_matches = matrix[uid]\n",
    "    top_matches = np.flip(np.argsort(all_matches))\n",
    "    match_rating = [[top_matches[i], all_matches[top_matches[i]]] for i in range(max_matches)]\n",
    "    return match_rating\n",
    "\n",
    "def print_similar_to_targets(samples, t_matrix):\n",
    "    for n in samples:\n",
    "        print(\"Target: \" + tid_name[n] + \" similar to:\")\n",
    "        matches = get_most_similar(n, t_matrix, t_max_matches)\n",
    "        if matches == None:\n",
    "            return\n",
    "        for item in matches[:10]:\n",
    "            tid, rating = item\n",
    "            print(tid_name[tid] + \" \" + \"%.4f\"%rating)\n",
    "        print()\n",
    "\n",
    "def print_similar_to_sources(samples, s_matrix):\n",
    "    for n in samples:\n",
    "        print(\"User: \" + sid_name[n] + \" similar to:\")\n",
    "        matches = get_most_similar(n, s_matrix, s_max_matches)\n",
    "        if matches == None:\n",
    "            return\n",
    "        for item in matches[:10]:\n",
    "            sid, rating = item\n",
    "            print(sid_name[sid] + \" \" + \"%.4f\"%rating)\n",
    "        print()\n",
    "        \n",
    "def print_target_similarity(t1, t2, t_matrix):\n",
    "    sim = t_matrix[t1][t2]\n",
    "    print(tid_name[t1] + \" similarity to \" + tid_name[t2] + \": \" + \"%.4f\"%sim)\n",
    "    \n",
    "def get_communities(inter):\n",
    "    mapping = []\n",
    "    for source, targets in inter.items():\n",
    "        for target, count in targets.items():\n",
    "            mapping.append((source, target, count))\n",
    "    g=nx.Graph()\n",
    "    g.add_weighted_edges_from(mapping)\n",
    "    communities = community.best_partition(g)\n",
    "\n",
    "    clusters = {}\n",
    "    for node, mod in communities.items():\n",
    "        if mod not in clusters:\n",
    "            clusters[mod] = []\n",
    "        clusters[mod].append(node)\n",
    "    return clusters\n",
    "\n",
    "def get_mean_distance(inter, target, source_list):\n",
    "    mapping = []\n",
    "    names = set()\n",
    "    for source, targets in inter.items():\n",
    "        names.add(source)\n",
    "        for target, count in targets.items():\n",
    "            mapping.append((source, target, count))\n",
    "            names.add(target)\n",
    "    g=nx.Graph()\n",
    "    g.add_weighted_edges_from(mapping)\n",
    "\n",
    "    distance_vals = []\n",
    "    for source in source_list:\n",
    "        if source in names:\n",
    "            length = nx.shortest_path_length(g, source=target, target=source)\n",
    "            distance_vals.append(length)\n",
    "    return np.mean(distance_vals)\n",
    "\n",
    "# Return a new poisoned dataframe\n",
    "def get_poisoned_dataset(ratings, amplifier_candidates, num_amplifiers, rating_val, save_path):\n",
    "    ratings2 = pd.DataFrame(ratings)\n",
    "    # For base set measurements\n",
    "    if num_amplifiers < 1 or rating_val < 1:\n",
    "        return ratings2\n",
    "    new_data = []\n",
    "    amplifiers = random.sample(amplifier_candidates, num_amplifiers)\n",
    "    for uid in amplifiers:\n",
    "        new_data.append([uid, target_tid, rating_val])\n",
    "        new_data.append([uid, high_profile_tid, rating_val])\n",
    "    new_ratings_df = pd.DataFrame(new_data, columns=['Source', 'Target', 'Weight'])\n",
    "    ratings2 = ratings2.append(new_ratings_df, ignore_index=True)\n",
    "    \n",
    "    # Save poisoned dataset for further inspection or visualization in gephi\n",
    "    interactions2 = {}\n",
    "    for item in zip(ratings2['Source'], ratings2['Target'], ratings2['Weight']):\n",
    "        s, t, r = item\n",
    "        sid_label = sid_name[s]\n",
    "        tid_label = tid_name[t]\n",
    "        if sid_label not in interactions2:\n",
    "            interactions2[sid_label] = Counter()\n",
    "        interactions2[sid_label][tid_label] += r\n",
    "    with open(save_path, \"w\") as f:\n",
    "        f.write(\"Source,Target,Weight\\n\")\n",
    "        for s, tw in interactions2.items():\n",
    "            for t, w in tw.items():\n",
    "                f.write(str(s)+\",\"+str(t)+\",\"+str(w)+\"\\n\")\n",
    "    return ratings2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare raw data\n",
    "raw = pd.read_csv(\"US2020/anonymized_interactions.csv\")\n",
    "\n",
    "# Source ids (accounts that retweeted)\n",
    "sid_name = {}\n",
    "name_sid = {}\n",
    "sid = 0\n",
    "for name in raw['Source']:\n",
    "    if name not in name_sid:\n",
    "        name_sid[name] = sid\n",
    "        sid_name[sid] = name\n",
    "        sid += 1\n",
    "\n",
    "# Target ids (accounts that received retweets)\n",
    "tid_name = {}\n",
    "name_tid = {}\n",
    "tid = 0\n",
    "for name in raw['Target']:\n",
    "    if name not in name_tid:\n",
    "        name_tid[name] = tid\n",
    "        tid_name[tid] = name\n",
    "        tid += 1\n",
    "\n",
    "print(\"Number of retweeters: \" + str(len(name_sid)))\n",
    "print(\"Number of retweeted: \" + str(len(name_tid)))\n",
    "# Assemble ratings dataframe used to train the model\n",
    "ratings = pd.DataFrame()\n",
    "ratings['Source'] = [name_sid[x] for x in raw['Source']]\n",
    "ratings['Target'] = [name_tid[x] for x in raw['Target']]\n",
    "ratings['Weight'] = raw['Weight']\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_list = list(set(ratings['Source']))\n",
    "target_list = list(set(ratings['Target']))\n",
    "target_retweeted_by = {}\n",
    "target_retweeted_count = {}\n",
    "target_retweeters = Counter()\n",
    "target_source_count = Counter()\n",
    "source_retweeted = {}\n",
    "source_retweets = Counter()\n",
    "source_target_count = Counter()\n",
    "interactions = {}\n",
    "for item in zip(ratings['Source'], ratings['Target'], ratings['Weight']):\n",
    "    s, t, r = item\n",
    "    if sid_name[s] not in interactions:\n",
    "        interactions[sid_name[s]] = Counter()\n",
    "    interactions[sid_name[s]][tid_name[t]] += r\n",
    "    source_retweets[s] += r\n",
    "    if t not in target_retweeted_count:\n",
    "        target_retweeted_count[t] = Counter()\n",
    "    target_retweeted_count[t][s] = r\n",
    "    if s not in source_retweeted:\n",
    "        source_retweeted[s] = []\n",
    "    if t not in source_retweeted[s]:\n",
    "        source_retweeted[s].append(t)\n",
    "        source_target_count[s] += 1\n",
    "    if t not in target_retweeted_by:\n",
    "        target_retweeted_by[t] = []\n",
    "    if s not in target_retweeted_by[t]:\n",
    "        target_retweeted_by[t].append(s)\n",
    "        target_source_count[t] += 1\n",
    "    target_retweeters[t] += 1\n",
    "with open(\"US2020/labeled_ratings.csv\", \"w\") as f:\n",
    "    f.write(\"Source,Target,Weight\\n\")\n",
    "    for s, tw in interactions.items():\n",
    "        for t, w in tw.items():\n",
    "            f.write(str(s)+\",\"+str(t)+\",\"+str(w)+\"\\n\")\n",
    "print(\"Number of sources: \" + str(len(source_list)))\n",
    "print(\"Number of targets: \" + str(len(target_list)))\n",
    "print(\"Total number of retweet interactions: \" + str(sum(ratings['Weight'])))\n",
    "print()\n",
    "print(\"Targets with most retweets\")\n",
    "print(\"tid\\tretweets\")\n",
    "for x, c in target_retweeters.most_common(10):\n",
    "    print(tid_name[x] + \"\\t\" + str(c))\n",
    "print()\n",
    "print(\"Targets with most unique sources retweeting them\")\n",
    "print(\"tid\\tsources\")\n",
    "for x, c in target_source_count.most_common(10):\n",
    "    print(tid_name[x] + \"\\t\" + str(c))\n",
    "print()\n",
    "for x, c in target_retweeters.most_common(10):\n",
    "    also_retweeted = Counter()\n",
    "    for sid, tids in source_retweeted.items():\n",
    "        if len(tids) > 1:\n",
    "            if x in tids:\n",
    "                for tid in tids:\n",
    "                    if tid != x:\n",
    "                        also_retweeted[tid] += 1\n",
    "    msg = \"Sources that retweeted \" + tid_name[x]\n",
    "    msg += \" also retweeted \" + str(len(also_retweeted)) + \" other accounts.\"\n",
    "    print(msg)\n",
    "    for x, c in also_retweeted.most_common(10):\n",
    "        print(\"Retweeted \" + tid_name[x] + \" \" + str(c) + \" times.\")\n",
    "    print(\"\")\n",
    "\n",
    "# People who retweeted x also retweeted y\n",
    "print()\n",
    "print(\"Sources that published the most retweets\")\n",
    "print(\"sid\\tretweets\")\n",
    "for x, c in source_retweets.most_common(10):\n",
    "    print(sid_name[x] + \"\\t\" + str(c))\n",
    "print()\n",
    "print(\"Sources that retweeted the most unique targets\")\n",
    "print(\"sid\\ttargets\")\n",
    "for x, c in source_target_count.most_common(10):\n",
    "    print(sid_name[x] + \"\\t\" + str(c))\n",
    "    \n",
    "communities = get_communities(interactions)\n",
    "community_sids = {}\n",
    "community_sizes = Counter()\n",
    "for mod, names in communities.items():\n",
    "    community_sizes[mod] = len(names)\n",
    "print(len(communities))\n",
    "print(community_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose accounts for poisoning experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here a target to be boosted and a high-profile account that the target should be seen as similar are picked\n",
    "# high_profile_tid was chosen from the original labeled dataset.\n",
    "# It is a high-profile Twitter account that gets a lot of engagement\n",
    "high_profile_tid = 191\n",
    "high_profile_name = tid_name[high_profile_tid]\n",
    "high_profile_sid = None\n",
    "if high_profile_name in name_sid:\n",
    "    high_profile_sid = name_sid[high_profile_name]\n",
    "print(\"high_profile_tid: \" + str(high_profile_tid) + \" == \" + tid_name[high_profile_tid])\n",
    "print(\"high_profile_sid: \" + str(high_profile_sid))\n",
    "# The target account was selected based on a few criteria:\n",
    "# - it is highly retweeted in the original dataset (top 10)\n",
    "# - the original dataset contains plenty of accounts that haven't retweeted it and high_profile_tid\n",
    "#   (thus enabling us to create a large number of amplifier candidates below)\n",
    "target_tid = 4451\n",
    "target_name = tid_name[target_tid]\n",
    "target_sid = None\n",
    "if target_name in name_sid:\n",
    "    target_sid = name_sid[target_name]\n",
    "print(\"target_tid: \" + str(target_tid) + \" == \" + tid_name[target_tid])\n",
    "print(\"target_sid: \" + str(target_sid))\n",
    "# Feel free to change these values for other experiments\n",
    "\n",
    "# Pick a list of accounts that engaged with the high profile account in order to compare\n",
    "# similarity values before and after poisoning\n",
    "num_controls = 20\n",
    "control_candidates = []\n",
    "for sid, tids in source_retweeted.items():\n",
    "    if len(tids) > 50:\n",
    "        if high_profile_tid in tids:\n",
    "            control_candidates.append(sid)\n",
    "print(\"Candidates for control accounts: \" + str(len(control_candidates)))\n",
    "controls = random.sample(control_candidates, num_controls)\n",
    "# For consistency's sake, here's a hard-coded list of control candidates \n",
    "# (selected by running the above code once)\n",
    "# controls = [229, 6266, 340, 124, 25, 4000, 89, 4347, 1947, 20144, 14, 22, 107, 13426, 237, 708, 1560, 62, 9, 11]\n",
    "print(controls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find accounts that engaged with both high_profile and target\n",
    "retweeted_target = target_retweeted_by[target_tid]\n",
    "print(\"Number of accounts that retweeted target:\")\n",
    "print(len(retweeted_target))\n",
    "retweeted_high_profile = target_retweeted_by[high_profile_tid]\n",
    "print(\"Number of accounts that retweeted high-profile:\")\n",
    "print(len(retweeted_high_profile))\n",
    "retweeted_both = set(retweeted_target).intersection(set(retweeted_high_profile))\n",
    "print(\"Number of accounts that retweeted both:\")\n",
    "print(len(retweeted_both))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Analysis of communities \n",
    "# Note: 7 (reduced effect) and 17 (improved effect)\n",
    "\n",
    "# Maybe try to extract features that characterise each community and then correlate these features (regression)\n",
    "# with attack success rate. This may reveal what characteristics of communities make for a successful attack.\n",
    "\n",
    "# What sort of features can be extracted from a community?\n",
    "# - distribution of retweet counts (perhaps into buckets)\n",
    "# - average cosine similarity between community members and target / high-profile\n",
    "\n",
    "retweeted_target = target_retweeted_by[target_tid]\n",
    "retweeted_target_count = target_retweeted_count[target_tid]\n",
    "retweeted_high_profile = target_retweeted_by[high_profile_tid]\n",
    "retweeted_high_profile_count = target_retweeted_count[high_profile_tid]\n",
    "\n",
    "num_amplifiers = 200\n",
    "community_features = {}\n",
    "for mod, names in communities.items():\n",
    "    amplifier_candidates = []\n",
    "    for name in names:\n",
    "        if name in name_sid:\n",
    "            sid = name_sid[name]\n",
    "            if sid in source_retweeted:\n",
    "                rtw = source_retweeted[sid]\n",
    "                if high_profile_tid not in rtw and target_tid not in rtw:\n",
    "                    amplifier_candidates.append(sid)\n",
    "    if len(amplifier_candidates) < num_amplifiers:\n",
    "        continue    \n",
    "    community_features[mod] = {}\n",
    "    community_features[mod][\"Community size\"] = len(names)\n",
    "    sid_list = [name_sid[x] for x in names if x in name_sid]\n",
    "    rtw_target_sids = set(retweeted_target).intersection(set(sid_list))\n",
    "    community_features[mod]['Unique accounts in community that retweeted target'] = len(rtw_target_sids)\n",
    "    rtw_target_count = 0\n",
    "    for sid, count in retweeted_target_count.items():\n",
    "        if sid in rtw_target_sids:\n",
    "            rtw_target_count += count\n",
    "    community_features[mod]['Total retweets of target by community'] = rtw_target_count\n",
    "    rtw_high_profile_sids = set(retweeted_high_profile).intersection(set(sid_list))\n",
    "    community_features[mod]['Unique accounts in community that retweeted high-profile'] = len(rtw_high_profile_sids)\n",
    "    rtw_high_profile_count = 0\n",
    "    for sid, count in retweeted_high_profile_count.items():\n",
    "        if sid in rtw_high_profile_sids:\n",
    "            rtw_high_profile_count += count\n",
    "    community_features[mod]['Total retweets of high-profile by community'] = rtw_high_profile_count\n",
    "    community_retweet_counts = [source_retweets[x] for x in sid_list]\n",
    "    community_features[mod]['Mean retweets per account in this community'] = np.mean(community_retweet_counts)\n",
    "    community_features[mod]['Max retweet count in this community'] = max(community_retweet_counts)\n",
    "    \n",
    "    controls_in_mod = set(sid_list).intersection(set(controls))\n",
    "    community_features[mod]['Number of control accounts in community'] = len(controls_in_mod)\n",
    "        \n",
    "    rtw_controls_sids = 0\n",
    "    rtw_controls_count = 0\n",
    "    for sid in controls:\n",
    "        if sid in target_retweeted_count:\n",
    "            rcl = target_retweeted_count[sid]\n",
    "            for s, c in rcl.items():\n",
    "                if s in sid_list:\n",
    "                    rtw_controls_sids += 1\n",
    "                    rtw_controls_count += c\n",
    "    community_features[mod]['Accounts in this community that retweeted control accounts'] = rtw_controls_sids\n",
    "    community_features[mod]['Total control account retweets published by this community'] = rtw_controls_count\n",
    "    \n",
    "    target_mean_path_len = get_mean_distance(interactions, target_name, names)\n",
    "    community_features[mod]['Mean path length between community nodes and target'] = target_mean_path_len\n",
    "    #high_profile_mean_path_len = get_mean_distance(interactions, high_profile_name, names)\n",
    "    #community_features[mod]['Mean path length between community nodes and high-profile'] = high_profile_mean_path_len\n",
    "\n",
    "print(json.dumps(community_features, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a ratings dataframe containing columns \"Source\", \"Target\", \"Weight\"\n",
    "# train a collaborative filtering model and return the target and source weight embeds\n",
    "def make_model_collab(ratings, epochs):\n",
    "    min_rating = min(ratings[\"Weight\"])\n",
    "    max_rating = max(ratings[\"Weight\"])\n",
    "    print(\"Min rating: \" + str(min_rating) + \" Max rating: \" + str(max_rating))\n",
    "    dls = CollabDataLoaders.from_df(ratings, item_name='Target', bs=64)\n",
    "    learn = collab_learner(dls, n_factors=50, y_range=(min_rating, max_rating))\n",
    "    learn.fit_one_cycle(epochs)\n",
    "    # Model weights\n",
    "    target_w = learn.model.weight(dls.classes['Target'], is_item=True)\n",
    "    source_w = learn.model.weight(dls.classes['Source'], is_item=False)\n",
    "    return target_w, source_w\n",
    "\n",
    "def make_model_nn(ratings, epochs):\n",
    "    min_rating = min(ratings[\"Weight\"])\n",
    "    max_rating = max(ratings[\"Weight\"])\n",
    "    print(\"Min rating: \" + str(min_rating) + \" Max rating: \" + str(max_rating))\n",
    "    dls = CollabDataLoaders.from_df(ratings, item_name='Target', bs=64)\n",
    "    learn = collab_learner(dls, use_nn=True, \n",
    "                           emb_szs={'userId': 50, 'movieId':50}, \n",
    "                           layers=[256, 128], y_range=(min_rating, max_rating))\n",
    "\n",
    "    learn.fit_one_cycle(epochs)\n",
    "    target_w = to_np(learn.model.embeds[1].weight[1:])\n",
    "    source_w = to_np(learn.model.embeds[0].weight[1:])\n",
    "    return target_w, source_w\n",
    "\n",
    "def make_model(ratings, model_type, epochs):\n",
    "    print(\"Model type: \" + model_type)\n",
    "    if model_type == \"nn\":\n",
    "        return make_model_nn(ratings, epochs)\n",
    "    else:\n",
    "        return make_model_collab(ratings, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "model_type = \"default\"\n",
    "\n",
    "# Train collab model on base dataset\n",
    "target_w, source_w = make_model(ratings, model_type, epochs)\n",
    "\n",
    "print(target_w.shape)\n",
    "print(source_w.shape)\n",
    "\n",
    "# Calculate cosine similarity matrix between all targets in the set\n",
    "t_matrix = cosine_similarity(target_w)\n",
    "                            \n",
    "print(t_matrix.shape)\n",
    "print()\n",
    "\n",
    "# Calculate cosine similarity matrix between all sources in the set\n",
    "s_matrix = cosine_similarity(source_w)\n",
    "\n",
    "print(s_matrix.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations by target similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t_max_matches = 100 # top_n matches when doing target similarity\n",
    "\n",
    "# Show closest matches to selected targets\n",
    "samples = [target_tid, high_profile_tid]\n",
    "print_similar_to_targets(samples, t_matrix)\n",
    "print_target_similarity(target_tid, high_profile_tid, t_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph tid similarity, to make a visualization\n",
    "tid_inter = {}\n",
    "threshold = 0.99\n",
    "print(len(tid_name))\n",
    "for tid, name in tid_name.items():\n",
    "    matches = get_most_similar(tid, t_matrix, 50)\n",
    "    if matches is not None:\n",
    "        for item in matches:\n",
    "            tid2, sim = item\n",
    "            name2 = tid_name[tid2]\n",
    "            if name != name2:\n",
    "                if sim >= threshold:\n",
    "                    if name not in tid_inter:\n",
    "                        tid_inter[name] = Counter()\n",
    "                    tid_inter[name][name2] = sim\n",
    "print(\"Saving\")\n",
    "with open(\"US2020/tid_inter.csv\", \"w\") as f:\n",
    "    f.write(\"Source,Target,Weight\\n\")\n",
    "    for source, targets in tid_inter.items():\n",
    "        for target, weight in targets.items():\n",
    "            f.write(str(source)+\",\"+str(target)+\",\"+str(weight)+\"\\n\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build recommendations for source based on who they've retweeted\n",
    "# For each target retweeted by a source, see if we have an entry in most_similar\n",
    "# If we do, add each item to the recommended counter\n",
    "# Assign the value to be the source's rating multiplied by the similarity score\n",
    "# We'll also record what the user has already retweeted so we can recommend a target they haven't yet retweeted\n",
    "\n",
    "# for target, num_retweets in get_source_retweets(source):\n",
    "#    for similar, similarity in get_most_similar(target):\n",
    "#        recommended[similar] += num_retweets * similarity\n",
    "\n",
    "def get_user_recommendations_by_target(ratings, sid, t_matrix):\n",
    "    s_ratings = ratings.loc[ratings['Source'] == sid]\n",
    "    s_ratings = s_ratings.sort_values(by=\"Weight\", ascending=False)\n",
    "    s_r = list(zip(s_ratings['Target'], s_ratings['Weight']))\n",
    "    recommended = Counter()\n",
    "    seen = set()\n",
    "    for item in s_r:\n",
    "        tid, trating = item\n",
    "        if tid > len(t_matrix):\n",
    "            continue\n",
    "        seen.add(tid)\n",
    "        matches = get_most_similar(tid, t_matrix, t_max_matches)\n",
    "        if matches != None:\n",
    "            for entry in matches:\n",
    "                t, r = entry\n",
    "                recommended[t] += r * trating\n",
    "\n",
    "    # Now we'll build a recomendations list that contains the highest scored items\n",
    "    # calculated above that the user hasn't already rated\n",
    "    seen_recommendations = Counter()\n",
    "    not_seen_recommendations = Counter()\n",
    "    for tid, score in recommended.most_common():\n",
    "        if len(seen_recommendations) >= 10 and len(not_seen_recommendations) >= 10:\n",
    "            break\n",
    "        if tid not in seen:\n",
    "            not_seen_recommendations[tid] = score\n",
    "        else:\n",
    "            seen_recommendations[tid] = score\n",
    "    return seen_recommendations, not_seen_recommendations\n",
    "\n",
    "def print_recommendations_by_target(sid, seen_recommendations, not_seen_recommendations):\n",
    "    s_ratings = ratings.loc[ratings['Source'] == sid]\n",
    "    s_ratings = s_ratings.sort_values(by=\"Weight\", ascending=False)\n",
    "    s_r = list(zip(s_ratings['Target'], s_ratings['Weight']))\n",
    "    num_ratings = len(s_r)\n",
    "\n",
    "    ind_rating = {}\n",
    "    for item in s_r:\n",
    "        ind, trating = item\n",
    "        ind_rating[ind] = trating\n",
    "\n",
    "    # Now let's print the output and see if it's sane\n",
    "    print(\"User: \" + sid_name[sid] + \" retweeted \" + str(num_ratings) + \" different accounts.\")\n",
    "    print()\n",
    "    top10 = []\n",
    "    for item in s_r[:10]:\n",
    "        tid, trating = item\n",
    "        top10.append(tid)\n",
    "        msg = \"Retweeted by user: \" + str(trating) + \" times, total retweets: \" + str(target_retweeters[tid]) \n",
    "        msg += \"\\t  \" + tid_name[tid]\n",
    "        print(msg)\n",
    "    print()\n",
    "    print(\"Recommended (seen):\")\n",
    "    for x, c in seen_recommendations.most_common(10):\n",
    "        msg = \"%.4f\"%c + \"\\t(retweeted by user: \" + str(ind_rating[x]) + \" times,\"\n",
    "        msg += \" total retweets: \" + str(target_retweeters[x]) + \")\" + \"\\t\" + tid_name[x]\n",
    "        if x == target_tid:\n",
    "            msg += \" [X]\"\n",
    "        if x in top10:\n",
    "            msg += \" [*]\"\n",
    "        print(msg)\n",
    "    print()\n",
    "    print(\"Recommended (not seen):\")\n",
    "    for x, c in not_seen_recommendations.most_common(10):\n",
    "        msg = \"%.4f\"%c + \"\\t\" + \" (total retweets: \" \n",
    "        msg += str(target_retweeters[x]) + \")\\t\" + tid_name[x]                  \n",
    "        if x == target_tid:\n",
    "            msg += \" [X]\"\n",
    "        if x in top10:\n",
    "            msg += \" [*]\"\n",
    "        print(msg)\n",
    "    print(\"=====================================================\")\n",
    "    print()\n",
    "\n",
    "def print_user_recommendations_by_target(ratings, sid, t_matrix):\n",
    "    seen, not_seen = get_user_recommendations_by_target(ratings, sid, t_matrix)\n",
    "    print_recommendations_by_target(sid, seen, not_seen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display target-based recommendations for control set\n",
    "for n in controls:\n",
    "    print_user_recommendations_by_target(ratings, n, t_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations by source similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_max_matches = 100 # top_n matches when doing source similarity\n",
    "\n",
    "# Print 10 closest sources for control set\n",
    "print_similar_to_sources(controls, s_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommendations based on source similarity\n",
    "# From the previously calculated source similarities, calculate recommendations thus:\n",
    "# For each similar source, obtain their list of target ratings\n",
    "# Record a counter for each target where we add a value: similarity * rating\n",
    "# Once we have a ranked list of recommendations, choose the top items\n",
    "# based on whether the original user has rated the target or not\n",
    "\n",
    "# for similar_source, similarity in get_most_similar(source):\n",
    "#     for target, num_retweets in get_source_retweets(similar_source):\n",
    "#         recommended[target] += similarity * num_retweets\n",
    "\n",
    "\n",
    "def get_user_recommendations_by_source(ratings, sid, s_matrix):\n",
    "    s_ratings = ratings.loc[ratings['Source'] == sid]\n",
    "    s_ratings = s_ratings.sort_values(by=\"Weight\", ascending=False)\n",
    "    s_r = list(zip(s_ratings['Target'], s_ratings['Weight']))\n",
    "    seen = set()\n",
    "    tid_rating = {}\n",
    "    for item in s_r:\n",
    "        tid, trating = item\n",
    "        tid_rating[tid] = trating\n",
    "        seen.add(tid)\n",
    "\n",
    "    recommended = Counter()\n",
    "    matches = get_most_similar(sid, s_matrix, s_max_matches)\n",
    "    if matches != None:\n",
    "        for item in matches:\n",
    "            sid, similarity = item\n",
    "            ur = ratings.loc[ratings['Source'] == sid]\n",
    "            ur = list(zip(ur['Target'], ur['Weight']))\n",
    "            for entry in ur:\n",
    "                tid, mr = entry\n",
    "                recommended[tid] += similarity * mr\n",
    "\n",
    "    # Now we'll build a recomendations list that contains the highest scored items\n",
    "    # calculated above that the user hasn't already rated\n",
    "    seen_recommendations = Counter()\n",
    "    not_seen_recommendations = Counter()\n",
    "    for tid, score in recommended.most_common():\n",
    "        if len(seen_recommendations) >= 10 and len(not_seen_recommendations) >= 10:\n",
    "            break\n",
    "        if tid not in seen:\n",
    "            not_seen_recommendations[tid] = score\n",
    "        else:\n",
    "            seen_recommendations[tid] = score\n",
    "    return seen_recommendations, not_seen_recommendations\n",
    "\n",
    "def print_recommendations_by_source(sid, seen_recommendations, not_seen_recommendations):\n",
    "    s_ratings = ratings.loc[ratings['Source'] == sid]\n",
    "    s_ratings = s_ratings.sort_values(by=\"Weight\", ascending=False)\n",
    "    s_r = list(zip(s_ratings['Target'], s_ratings['Weight']))\n",
    "    num_ratings = len(s_r)\n",
    "\n",
    "    tid_rating = {}\n",
    "    for item in s_r:\n",
    "        tid, mrating = item\n",
    "        tid_rating[tid] = mrating\n",
    "\n",
    "    # Now let's print the output and see if it's sane\n",
    "    print(\"User: \" + sid_name[sid] + \" retweeted \" + str(num_ratings) + \" different accounts.\")\n",
    "    print()\n",
    "    top10 = []\n",
    "    for item in s_r[:10]:\n",
    "        tid, trating = item\n",
    "        top10.append(tid)\n",
    "        msg = \"Retweeted by user: \" + str(trating) + \" times, total retweets: \" + str(target_retweeters[tid]) \n",
    "        msg += \"\\t  \" + tid_name[tid]\n",
    "        print(msg)\n",
    "    print()\n",
    "    print(\"Recommended (seen):\")\n",
    "    for x, c in seen_recommendations.most_common(10):\n",
    "        msg = \"%.4f\"%c\n",
    "        msg += \" (retweeted by user: \" + str(tid_rating[x]) + \" times,\"\n",
    "        msg += \" total retweets: \" + str(target_retweeters[x]) + \")\" + \"\\t\" \n",
    "        msg += tid_name[x]\n",
    "        if x == target_tid:\n",
    "            msg += \" [X]\"\n",
    "        if x in top10:\n",
    "            msg += \" [*]\"\n",
    "        print(msg)\n",
    "    print()\n",
    "    print(\"Recommended (not seen):\")\n",
    "    for x, c in not_seen_recommendations.most_common(10):\n",
    "        msg = \"%.4f\"%c \n",
    "        msg += \" (total retweets: \" + str(target_retweeters[x]) + \")\\t\"\n",
    "        msg += tid_name[x]\n",
    "        if x == target_tid:\n",
    "            msg += \" [X]\"\n",
    "        if x in top10:\n",
    "            msg += \" [*]\"\n",
    "        print(msg)\n",
    "    print(\"=====================================================\")\n",
    "    print()\n",
    "\n",
    "def print_user_recommendations_by_source(ratings, sid, s_matrix):\n",
    "    seen, not_seen = get_user_recommendations_by_source(ratings, sid, s_matrix)\n",
    "    print_recommendations_by_source(sid, seen, not_seen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Print source-based recommendations for control set\n",
    "for n in controls:\n",
    "    print_user_recommendations_by_source(ratings, n, s_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation: count how many times target appeared in top_n recommendations for each source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_target_target(ratings, sid, t_matrix, target, top_n):\n",
    "    ret = False\n",
    "    seen, not_seen = get_user_recommendations_by_target(ratings, sid, t_matrix)\n",
    "    top_tids = [x for x, c in not_seen.most_common(top_n)]\n",
    "    if target in top_tids:\n",
    "        ret = True\n",
    "    return ret\n",
    "\n",
    "def validate_target_source(ratings, sid, s_matrix, target, top_n):\n",
    "    ret = False\n",
    "    seen, not_seen = get_user_recommendations_by_source(ratings, sid, s_matrix)\n",
    "    top_tids = [x for x, c in not_seen.most_common(top_n)]\n",
    "    if target in top_tids:\n",
    "        ret = True\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = 0\n",
    "top_n = 3\n",
    "for sid in controls:\n",
    "    found = validate_target_target(ratings, sid, t_matrix, target_tid, top_n)\n",
    "    if found == True:\n",
    "        ret += 1\n",
    "print(\"Target was in top \"+str(top_n)+\" target-based recommendations for \"+str(ret)+\" users in control list.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = 0\n",
    "top_n = 3\n",
    "for sid in controls:\n",
    "    found = validate_target_source(ratings, sid, s_matrix, target_tid, top_n)\n",
    "    if found == True:\n",
    "        res += 1\n",
    "print(\"Target was in top \"+str(top_n)+\" source-based recommendations for \"+str(res)+\" users in control list.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisoning experiment 1 - randomly chosen amplifiers, variable amps, retweets\n",
    "- with differing numbers of amplifiers and retweets:\n",
    "    - repeat \"iterations\" times\n",
    "        - create new poisoned dataframe based on supplied parameters\n",
    "        - save csv for gephi visualization\n",
    "        - train model\n",
    "        - run source-based and target-based recommendations, see how often target appears in top_n recommendations\n",
    "        - record all results to be graphed later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run poisoning experiment 1\n",
    "# Note that this cell can take a number of hours to run\n",
    "\n",
    "# These are the experiments to run\n",
    "# Each pair of numbers denotes [num_amplifiers, num_retweets]\n",
    "# A poisoned copy of the dataset is generated as follows:\n",
    "# 1. Make a copy of the original dataset\n",
    "# 2. Randomly select num_amplifiers accounts from amplifier candidates\n",
    "# 3. For each selected amplifier, add two rows to the copied dataset:\n",
    "#    amplifier - target - num_retweets\n",
    "#    amplifier - high_profile_user - num_retweets\n",
    "# 4. Perform the rest of the experiment (train model, analyze recomendations)\n",
    "experiments = [[0,0],\n",
    "               [10,10], [10,20], [10,50], [10,100],\n",
    "               [20,10], [20,20], [20,50], [20,100],\n",
    "               [50,10], [50,20], [50,50], [50,100],\n",
    "               [100,1], [100,5], [100,10], [100,20], [100,50],\n",
    "               [200,1], [200,5], [200,10], [200,20], [200,50],\n",
    "               [500,1], [500,5], [500,10], [500,20], [500,50],\n",
    "               [1000,1], [1000,5], [1000,10], [1000,20], [1000,50],\n",
    "               [2000,1], [2000,5], [2000,10], [2000,20], [2000,50],\n",
    "               [4000,1], [4000,5], [4000,10], [4000,20], [4000,50]]\n",
    "\n",
    "samples = [target_tid, high_profile_tid]\n",
    "top_n = 3\n",
    "iterations = 10\n",
    "epochs = 5\n",
    "\n",
    "# 1. Pick random accounts (not in control set) to do the boosting \n",
    "# that havent engaged with either high profile or target\n",
    "amplifier_candidates = []\n",
    "for sid, tids in source_retweeted.items():\n",
    "    if len(tids) > 0:\n",
    "        inter = set(tids).intersection(set(controls))\n",
    "        if len(inter) == 0:\n",
    "            if high_profile_tid not in tids and target_tid not in tids:\n",
    "                amplifier_candidates.append(sid)\n",
    "print(\"Number of random amplifier candidates: \" + str(len(amplifier_candidates)))\n",
    "\n",
    "# Loop through the experiment parameters\n",
    "# For each set of parameters, perform the experiment iterations number of times\n",
    "result_source = []\n",
    "result_target = []\n",
    "i = 1\n",
    "save_dir = \"US2020/exp1\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "for item in experiments:\n",
    "    amps, r = item\n",
    "    for n in range(iterations):\n",
    "        print()\n",
    "        print(\"Experiment:\" + str(i) + \" amps:\" + str(amps) + \" r:\" + str(r) + \" take:\" + str(n))\n",
    "        i = i + 1\n",
    "        save_path = save_dir + \"/\" + str(amps) + \"_\" + str(r) + \"_\" + str(n) + \".csv\"\n",
    "        new_ratings = get_poisoned_dataset(ratings, amplifier_candidates, amps, r, save_path)\n",
    "        msg = \"Base dataset length: \" + str(len(ratings))\n",
    "        msg += \" Poisoned dataset length: \" + str(len(new_ratings))\n",
    "        print(msg)\n",
    "        new_target_w, new_source_w = make_model(new_ratings, model_type, epochs)\n",
    "        new_t_matrix = cosine_similarity(new_target_w)\n",
    "        print_target_similarity(target_tid, high_profile_tid, new_t_matrix)\n",
    "        new_s_matrix = cosine_similarity(new_source_w)\n",
    "        ret = 0\n",
    "        for sid in controls:\n",
    "            found = validate_target_target(new_ratings, sid, new_t_matrix, target_tid, top_n)\n",
    "            if found == True:\n",
    "                ret += 1\n",
    "        msg = \"Target was in top \" + str(top_n) \n",
    "        msg += \" target recommendations for \" + str(ret) \n",
    "        msg += \" users in control list.\"\n",
    "        print(msg)\n",
    "        result_target.append([amps, r, n, ret])\n",
    "        with open(save_dir + \"/result_target.json\", \"w\") as f:\n",
    "            f.write(json.dumps(result_target, indent=4))\n",
    "        res = 0\n",
    "        for sid in controls:\n",
    "            found = validate_target_source(new_ratings, sid, new_s_matrix, target_tid, top_n)\n",
    "            if found == True:\n",
    "                res += 1\n",
    "        msg = \"Target was in top \" + str(top_n) \n",
    "        msg += \" source recommendations for \" + str(res) \n",
    "        msg += \" users in control list.\"\n",
    "        print(msg)\n",
    "        result_source.append([amps, r, n, res])\n",
    "        with open(save_dir + \"/result_source.json\", \"w\") as f:\n",
    "            f.write(json.dumps(result_source, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display results as a plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"US2020/exp1/result_source.json\"\n",
    "title = \"US2020 Experiment 1 - source-based recommendations\"\n",
    "results = []\n",
    "with open(filename, \"r\") as f:\n",
    "    results = json.loads(f.read())\n",
    "results2 = []\n",
    "order = []\n",
    "for item in results:\n",
    "    if len(item) == 4:\n",
    "        a, r, t, v = item\n",
    "        l = str(a) + \"_\" + str(r)\n",
    "    else:\n",
    "        m, t, v = item\n",
    "        l = str(m)\n",
    "    v = (v/20)*100\n",
    "    if l not in order:\n",
    "        order.append(l)\n",
    "    results2.append([l, t, v])\n",
    "df = pd.DataFrame(results2, columns=[\"params\", \"take\", \"val\"])\n",
    "\n",
    "plt.figure()\n",
    "ax = None\n",
    "fig = plt.figure(figsize=(20,8))\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.barplot(x=\"params\", y=\"val\", data=df, order=order, capsize=.2)\n",
    "ax.set_title(title)\n",
    "xlab = \"Experiment parameters (num_accounts, num_retweets)\"\n",
    "ylab = \"Percentage of control set that saw target account in top-3 recommendations\"\n",
    "plt.xlabel(xlab)\n",
    "plt.ylabel(ylab)\n",
    "for item in ax.get_xticklabels():\n",
    "    item.set_rotation(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"US2020/exp1/result_target.json\"\n",
    "title = \"US2020 Experiment 1 - target-based recommendations\"\n",
    "results = []\n",
    "with open(filename, \"r\") as f:\n",
    "    results = json.loads(f.read())\n",
    "results2 = []\n",
    "order = []\n",
    "for item in results:\n",
    "    if len(item) == 4:\n",
    "        a, r, t, v = item\n",
    "        l = str(a) + \"_\" + str(r)\n",
    "    else:\n",
    "        m, t, v = item\n",
    "        l = str(m)\n",
    "    v = (v/20)*100\n",
    "    if l not in order:\n",
    "        order.append(l)\n",
    "    results2.append([l, t, v])\n",
    "df = pd.DataFrame(results2, columns=[\"params\", \"take\", \"val\"])\n",
    "\n",
    "plt.figure()\n",
    "ax = None\n",
    "fig = plt.figure(figsize=(20,8))\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.barplot(x=\"params\", y=\"val\", data=df, order=order, capsize=.2)\n",
    "ax.set_title(title)\n",
    "xlab = \"Experiment parameters (num_accounts, num_retweets)\"\n",
    "ylab = \"Percentage of control set that saw target account in top-3 recommendations\"\n",
    "plt.xlabel(xlab)\n",
    "plt.ylabel(ylab)\n",
    "for item in ax.get_xticklabels():\n",
    "    item.set_rotation(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisoning experiment 2 - amplifiers chosen based on community\n",
    "- with fixed number of amplifiers and retweets:\n",
    "    - iterate through communities (discovered from louvain method)\n",
    "    - if a community contains at least num_amplifiers, select a set of amplifiers randomly from the community\n",
    "    - repeat \"iterations\" times\n",
    "        - create new poisoned dataframe based on supplied parameters\n",
    "        - train model\n",
    "        - run source-based and target-based recommendations, see how often target appears in top_n recommendations\n",
    "        - record all results to be graphed later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run poisoning experiment 2\n",
    "# Note that this cell can take a number of hours to run\n",
    "num_amplifiers = 200\n",
    "num_retweets = 20\n",
    "\n",
    "samples = [target_tid, high_profile_tid]\n",
    "top_n = 3\n",
    "iterations = 10\n",
    "epochs = 5\n",
    "\n",
    "result_source = []\n",
    "result_target = []\n",
    "i = 1\n",
    "save_dir = \"US2020/exp2\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "target_name = tid_name[target_tid]\n",
    "high_profile_name = tid_name[high_profile_tid]\n",
    "print(\"Community labels: \" + \", \".join([str(x) for x in communities.keys()]))\n",
    "for mod, names in communities.items():\n",
    "    if target_name in names:\n",
    "        print(\"target: \" + target_name + \" in community: \" + str(mod) + \" size: \" + str(len(names)))\n",
    "    if high_profile_name in names:\n",
    "        print(\"high_profile: \" + high_profile_name + \" in community: \" + str(mod) + \" size: \" + str(len(names)))\n",
    "\n",
    "for mod, names in sorted(communities.items()):\n",
    "    print(\"Community: \" + str(mod) + \" contains \" + str(len(names)) + \" names.\")\n",
    "    amplifier_candidates = []\n",
    "    for name in names:\n",
    "        if name in name_sid:\n",
    "            sid = name_sid[name]\n",
    "            if sid in source_retweeted:\n",
    "                rtw = source_retweeted[sid]\n",
    "                if high_profile_tid not in rtw and target_tid not in rtw:\n",
    "                    amplifier_candidates.append(sid)\n",
    "    if len(amplifier_candidates) < num_amplifiers:\n",
    "        print(\"Skipping community: \"+str(mod)+\" (only found \"+str(len(amplifier_candidates))+\" candidates).\")\n",
    "        continue\n",
    "    for n in range(iterations):\n",
    "        print()\n",
    "        print(\"Experiment:\" + str(i) + \" community:\" + str(mod) + \" take:\" + str(n))\n",
    "        i = i + 1\n",
    "        save_path = save_dir + \"/\" + str(mod) + \"_\" + str(n) + \".csv\"\n",
    "        new_ratings = get_poisoned_dataset(ratings, amplifier_candidates, \n",
    "                                           num_amplifiers, num_retweets, save_path)\n",
    "        msg = \"Base dataset length: \" + str(len(ratings))\n",
    "        msg += \" Poisoned dataset length: \" + str(len(new_ratings))\n",
    "        print(msg)\n",
    "        new_target_w, new_source_w = make_model(new_ratings, model_type, epochs)\n",
    "        new_t_matrix = cosine_similarity(new_target_w)\n",
    "        print_target_similarity(target_tid, high_profile_tid, new_t_matrix)\n",
    "        new_s_matrix = cosine_similarity(new_source_w)\n",
    "        ret = 0\n",
    "        for sid in controls:\n",
    "            found = validate_target_target(new_ratings, sid, new_t_matrix, target_tid, top_n)\n",
    "            if found == True:\n",
    "                ret += 1\n",
    "        msg = \"Target was in top \" + str(top_n) \n",
    "        msg += \" target recommendations for \" + str(ret) \n",
    "        msg += \" users in control list.\"\n",
    "        print(msg)\n",
    "        result_target.append([mod, n, ret])\n",
    "        with open(save_dir + \"/result_target.json\", \"w\") as f:\n",
    "            f.write(json.dumps(result_target, indent=4))\n",
    "        res = 0\n",
    "        for sid in controls:\n",
    "            found = validate_target_source(new_ratings, sid, new_s_matrix, target_tid, top_n)\n",
    "            if found == True:\n",
    "                res += 1\n",
    "        msg = \"Target was in top \" + str(top_n) \n",
    "        msg += \" source recommendations for \" + str(res) \n",
    "        msg += \" users in control list.\"\n",
    "        print(msg)\n",
    "        result_source.append([mod, n, res])\n",
    "        with open(save_dir + \"/result_source.json\", \"w\") as f:\n",
    "            f.write(json.dumps(result_source, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"US2020/exp2/result_source.json\"\n",
    "title = \"US2020 Experiment 2 - source-based recommendations\"\n",
    "results = []\n",
    "with open(filename, \"r\") as f:\n",
    "    results = json.loads(f.read())\n",
    "results2 = []\n",
    "order = []\n",
    "for item in results:\n",
    "    if len(item) == 4:\n",
    "        a, r, t, v = item\n",
    "        l = str(a) + \"_\" + str(r)\n",
    "    else:\n",
    "        m, t, v = item\n",
    "        l = str(m)\n",
    "    v = (v/20)*100\n",
    "    if l not in order:\n",
    "        order.append(l)\n",
    "    results2.append([l, t, v])\n",
    "df = pd.DataFrame(results2, columns=[\"params\", \"take\", \"val\"])\n",
    "\n",
    "plt.figure()\n",
    "ax = None\n",
    "fig = plt.figure(figsize=(20,8))\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.barplot(x=\"params\", y=\"val\", data=df, order=order, capsize=.2)\n",
    "ax.set_title(title)\n",
    "xlab = \"Experiment parameters (community label)\"\n",
    "ylab = \"Percentage of control set that saw target account in top-3 recommendations\"\n",
    "plt.xlabel(xlab)\n",
    "plt.ylabel(ylab)\n",
    "for item in ax.get_xticklabels():\n",
    "    item.set_rotation(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"US2020/exp2/result_target.json\"\n",
    "title = \"US2020 Experiment 2 - target-based recommendations\"\n",
    "results = []\n",
    "with open(filename, \"r\") as f:\n",
    "    results = json.loads(f.read())\n",
    "results2 = []\n",
    "order = []\n",
    "for item in results:\n",
    "    if len(item) == 4:\n",
    "        a, r, t, v = item\n",
    "        l = str(a) + \"_\" + str(r)\n",
    "    else:\n",
    "        m, t, v = item\n",
    "        l = str(m)\n",
    "    v = (v/20)*100\n",
    "    if l not in order:\n",
    "        order.append(l)\n",
    "    results2.append([l, t, v])\n",
    "df = pd.DataFrame(results2, columns=[\"params\", \"take\", \"val\"])\n",
    "\n",
    "plt.figure()\n",
    "ax = None\n",
    "fig = plt.figure(figsize=(20,8))\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.barplot(x=\"params\", y=\"val\", data=df, order=order, capsize=.2)\n",
    "ax.set_title(title)\n",
    "xlab = \"Experiment parameters (community label)\"\n",
    "ylab = \"Percentage of control set that saw target account in top-3 recommendations\"\n",
    "plt.xlabel(xlab)\n",
    "plt.ylabel(ylab)\n",
    "for item in ax.get_xticklabels():\n",
    "    item.set_rotation(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisoning experiment 3 - amplifiers chosen based on similarity to control accounts\n",
    "- with varying number of amplifiers and retweets:\n",
    "    - select a set of amplifiers that are similar to control accounts\n",
    "    - repeat \"iterations\" times\n",
    "        - create new poisoned dataframe based on supplied parameters\n",
    "        - train model\n",
    "        - run source-based and target-based recommendations, see how often target appears in top_n recommendations\n",
    "        - record all results to be graphed later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run poisoning experiment 3\n",
    "# Note that this cell can take a number of hours to run\n",
    "\n",
    "# These are the experiments to run\n",
    "# Each pair of numbers denotes [num_amplifiers, num_retweets]\n",
    "# A poisoned copy of the dataset is generated as follows:\n",
    "# 1. Make a copy of the original dataset\n",
    "# 2. Randomly select num_amplifiers accounts from amplifier candidates\n",
    "# 3. For each selected amplifier, add two rows to the copied dataset:\n",
    "#    amplifier - target - num_retweets\n",
    "#    amplifier - high_profile_user - num_retweets\n",
    "# 4. Perform the rest of the experiment (train model, analyze recomendations)\n",
    "experiments = [[0,0],\n",
    "               [100,1], [100,5], [100,10], [100,20],\n",
    "               [200,1], [200,5], [200,10], [200,20], \n",
    "               [500,1], [500,5], [500,10], [500,20], \n",
    "               [1000,1], [1000,5], [1000,10], [1000,20],\n",
    "               [2000,1], [2000,5], [2000,10], [2000,20]]\n",
    "\n",
    "samples = [target_tid, high_profile_tid]\n",
    "top_n = 3\n",
    "iterations = 10\n",
    "epochs = 5\n",
    "\n",
    "# 1. Pick accounts most similar to those in the control set\n",
    "# that havent engaged with either high profile or target\n",
    "# and aren't in the control group\n",
    "amplifier_candidates = []\n",
    "sims = set()\n",
    "for sid in controls:\n",
    "    sim = get_most_similar(sid, s_matrix, 250)\n",
    "    for s, _ in sim:\n",
    "        if s not in controls:\n",
    "            if s in source_retweeted:\n",
    "                rtw = source_retweeted[s]\n",
    "                if high_profile_tid not in rtw and target_tid not in rtw:\n",
    "                    sims.add(s)\n",
    "amplifier_candidates = list(sims)\n",
    "print(\"Number of amplifier candidates: \" + str(len(amplifier_candidates)))\n",
    "# Loop through the experiment parameters\n",
    "# For each set of parameters, perform the experiment iterations number of times\n",
    "result_source = []\n",
    "result_target = []\n",
    "i = 1\n",
    "save_dir = \"US2020/exp3\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "for item in experiments:\n",
    "    amps, r = item\n",
    "    for n in range(iterations):\n",
    "        print()\n",
    "        print(\"Experiment:\" + str(i) + \" amps:\" + str(amps) + \" r:\" + str(r) + \" take:\" + str(n))\n",
    "        i = i + 1\n",
    "        save_path = save_dir + \"/\" + str(amps) + \"_\" + str(r) + \"_\" + str(n) + \".csv\"\n",
    "        new_ratings = get_poisoned_dataset(ratings, amplifier_candidates, amps, r, save_path)\n",
    "        msg = \"Base dataset length: \" + str(len(ratings))\n",
    "        msg += \" Poisoned dataset length: \" + str(len(new_ratings))\n",
    "        print(msg)\n",
    "        new_target_w, new_source_w = make_model(new_ratings, model_type, epochs)\n",
    "        new_t_matrix = cosine_similarity(new_target_w)\n",
    "        print_target_similarity(target_tid, high_profile_tid, new_t_matrix)\n",
    "        new_s_matrix = cosine_similarity(new_source_w)\n",
    "        ret = 0\n",
    "        for sid in controls:\n",
    "            found = validate_target_target(new_ratings, sid, new_t_matrix, target_tid, top_n)\n",
    "            if found == True:\n",
    "                ret += 1\n",
    "        msg = \"Target was in top \" + str(top_n) \n",
    "        msg += \" target recommendations for \" + str(ret) \n",
    "        msg += \" users in control list.\"\n",
    "        print(msg)\n",
    "        result_target.append([amps, r, n, ret])\n",
    "        with open(save_dir + \"/result_target.json\", \"w\") as f:\n",
    "            f.write(json.dumps(result_target, indent=4))\n",
    "        res = 0\n",
    "        for sid in controls:\n",
    "            found = validate_target_source(new_ratings, sid, new_s_matrix, target_tid, top_n)\n",
    "            if found == True:\n",
    "                res += 1\n",
    "        msg = \"Target was in top \" + str(top_n) \n",
    "        msg += \" source recommendations for \" + str(res) \n",
    "        msg += \" users in control list.\"\n",
    "        print(msg)\n",
    "        result_source.append([amps, r, n, res])\n",
    "        with open(save_dir + \"/result_source.json\", \"w\") as f:\n",
    "            f.write(json.dumps(result_source, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"US2020/exp3/result_source.json\"\n",
    "title = \"US2020 Experiment 3 - source-based recommendations\"\n",
    "results = []\n",
    "with open(filename, \"r\") as f:\n",
    "    results = json.loads(f.read())\n",
    "results2 = []\n",
    "order = []\n",
    "for item in results:\n",
    "    if len(item) == 4:\n",
    "        a, r, t, v = item\n",
    "        l = str(a) + \"_\" + str(r)\n",
    "    else:\n",
    "        m, t, v = item\n",
    "        l = str(m)\n",
    "    v = (v/20)*100\n",
    "    if l not in order:\n",
    "        order.append(l)\n",
    "    results2.append([l, t, v])\n",
    "df = pd.DataFrame(results2, columns=[\"params\", \"take\", \"val\"])\n",
    "\n",
    "plt.figure()\n",
    "ax = None\n",
    "fig = plt.figure(figsize=(20,8))\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.barplot(x=\"params\", y=\"val\", data=df, order=order, capsize=.2)\n",
    "ax.set_title(title)\n",
    "xlab = \"Experiment parameters (num_accounts, num_retweets)\"\n",
    "ylab = \"Percentage of control set that saw target account in top-3 recommendations\"\n",
    "plt.xlabel(xlab)\n",
    "plt.ylabel(ylab)\n",
    "for item in ax.get_xticklabels():\n",
    "    item.set_rotation(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"US2020/exp3/result_target.json\"\n",
    "title = \"US2020 Experiment 3 - target-based recommendations\"\n",
    "results = []\n",
    "with open(filename, \"r\") as f:\n",
    "    results = json.loads(f.read())\n",
    "results2 = []\n",
    "order = []\n",
    "for item in results:\n",
    "    if len(item) == 4:\n",
    "        a, r, t, v = item\n",
    "        l = str(a) + \"_\" + str(r)\n",
    "    else:\n",
    "        m, t, v = item\n",
    "        l = str(m)\n",
    "    v = (v/20)*100\n",
    "    if l not in order:\n",
    "        order.append(l)\n",
    "    results2.append([l, t, v])\n",
    "df = pd.DataFrame(results2, columns=[\"params\", \"take\", \"val\"])\n",
    "\n",
    "plt.figure()\n",
    "ax = None\n",
    "fig = plt.figure(figsize=(20,8))\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.barplot(x=\"params\", y=\"val\", data=df, order=order, capsize=.2)\n",
    "ax.set_title(title)\n",
    "xlab = \"Experiment parameters (num_accounts, num_retweets)\"\n",
    "ylab = \"Percentage of control set that saw target account in top-3 recommendations\"\n",
    "plt.xlabel(xlab)\n",
    "plt.ylabel(ylab)\n",
    "for item in ax.get_xticklabels():\n",
    "    item.set_rotation(45)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
